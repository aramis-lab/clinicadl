<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>clinicadl.generate.generate API documentation</title>
<meta name="description" content="This file generates data for trivial or intractable (random) data for binary classification." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>clinicadl.generate.generate</code></h1>
</header>
<section id="section-intro">
<p>This file generates data for trivial or intractable (random) data for binary classification.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># coding: utf8

&#34;&#34;&#34;
This file generates data for trivial or intractable (random) data for binary classification.
&#34;&#34;&#34;
import tarfile
from os import makedirs
from os.path import basename, dirname, exists, join
from typing import Dict, Optional, Tuple

import nibabel as nib
import numpy as np
import pandas as pd
import torch
from clinica.utils.inputs import RemoteFileStructure, clinica_file_reader, fetch_file

from clinicadl.prepare_data.prepare_data_utils import compute_extract_json
from clinicadl.utils.caps_dataset.data import CapsDataset
from clinicadl.utils.maps_manager.iotools import check_and_clean, commandline_to_json
from clinicadl.utils.preprocessing import write_preprocessing
from clinicadl.utils.tsvtools_utils import extract_baseline

from .generate_utils import (
    find_file_type,
    generate_shepplogan_phantom,
    im_loss_roi_gaussian_distribution,
    load_and_check_tsv,
    write_missing_mods,
)


def generate_random_dataset(
    caps_directory: str,
    output_dir: str,
    n_subjects: int,
    tsv_path: Optional[str] = None,
    mean: float = 0,
    sigma: float = 0.5,
    preprocessing: str = &#34;t1-linear&#34;,
    multi_cohort: bool = False,
    uncropped_image: bool = False,
    acq_label: Optional[str] = None,
    suvr_reference_region: Optional[str] = None,
):
    &#34;&#34;&#34;
    Generates a random dataset.

    Creates a random dataset for intractable classification task from the first
    subject of the tsv file (other subjects/sessions different from the first
    one are ignored. Degree of noise can be parameterized.

    Args:
        caps_directory: Path to the (input) CAPS directory.
        output_dir: folder containing the synthetic dataset in (output)
            CAPS format.
        n_subjects: number of subjects in each class of the
            synthetic dataset
        tsv_path: path to tsv file of list of subjects/sessions.
        mean: mean of the gaussian noise
        sigma: standard deviation of the gaussian noise
        preprocessing: preprocessing performed. Must be in [&#39;t1-linear&#39;, &#39;t1-extensive&#39;].
        multi_cohort: If True caps_directory is the path to a TSV file linking cohort names and paths.
        uncropped_image: If True the uncropped image of `t1-linear` or `pet-linear` will be used.
        acq_label: name of the tracer when using `pet-linear` preprocessing.
        suvr_reference_region: name of the reference region when using `pet-linear` preprocessing.

    Returns:
        A folder written on the output_dir location (in CAPS format), also a
        tsv file describing this output

    &#34;&#34;&#34;
    commandline_to_json(
        {
            &#34;output_dir&#34;: output_dir,
            &#34;caps_dir&#34;: caps_directory,
            &#34;preprocessing&#34;: preprocessing,
            &#34;n_subjects&#34;: n_subjects,
            &#34;mean&#34;: mean,
            &#34;sigma&#34;: sigma,
        }
    )
    # Transform caps_directory in dict
    caps_dict = CapsDataset.create_caps_dict(caps_directory, multi_cohort=multi_cohort)

    # Read DataFrame
    data_df = load_and_check_tsv(tsv_path, caps_dict, output_dir)

    # Create subjects dir
    makedirs(join(output_dir, &#34;subjects&#34;), exist_ok=True)

    # Retrieve image of first subject
    participant_id = data_df.loc[0, &#34;participant_id&#34;]
    session_id = data_df.loc[0, &#34;session_id&#34;]
    cohort = data_df.loc[0, &#34;cohort&#34;]

    # Find appropriate preprocessing file type
    file_type = find_file_type(
        preprocessing, uncropped_image, acq_label, suvr_reference_region
    )

    image_paths = clinica_file_reader(
        [participant_id], [session_id], caps_dict[cohort], file_type
    )[0]
    image_nii = nib.load(image_paths[0])
    image = image_nii.get_data()

    # Create output tsv file
    participant_id_list = [f&#34;sub-RAND{i}&#34; for i in range(2 * n_subjects)]
    session_id_list = [&#34;ses-M00&#34;] * 2 * n_subjects
    diagnosis_list = [&#34;AD&#34;] * n_subjects + [&#34;CN&#34;] * n_subjects
    data = np.array([participant_id_list, session_id_list, diagnosis_list])
    data = data.T
    output_df = pd.DataFrame(
        data, columns=[&#34;participant_id&#34;, &#34;session_id&#34;, &#34;diagnosis&#34;]
    )
    output_df[&#34;age_bl&#34;] = 60
    output_df[&#34;sex&#34;] = &#34;F&#34;
    output_df.to_csv(join(output_dir, &#34;data.tsv&#34;), sep=&#34;\t&#34;, index=False)

    input_filename = basename(image_paths[0])
    filename_pattern = &#34;_&#34;.join(input_filename.split(&#34;_&#34;)[2::])
    for i in range(2 * n_subjects):
        gauss = np.random.normal(mean, sigma, image.shape)
        participant_id = f&#34;sub-RAND{i}&#34;
        noisy_image = image + gauss
        noisy_image_nii = nib.Nifti1Image(
            noisy_image, header=image_nii.header, affine=image_nii.affine
        )
        noisy_image_nii_path = join(
            output_dir, &#34;subjects&#34;, participant_id, &#34;ses-M00&#34;, &#34;t1_linear&#34;
        )
        noisy_image_nii_filename = f&#34;{participant_id}_ses-M00_{filename_pattern}&#34;
        makedirs(noisy_image_nii_path, exist_ok=True)
        nib.save(noisy_image_nii, join(noisy_image_nii_path, noisy_image_nii_filename))

    write_missing_mods(output_dir, output_df)


def generate_trivial_dataset(
    caps_directory: str,
    output_dir: str,
    n_subjects: int,
    tsv_path: Optional[str] = None,
    preprocessing: str = &#34;t1-linear&#34;,
    mask_path: Optional[str] = None,
    atrophy_percent: float = 60,
    multi_cohort: bool = False,
    uncropped_image: bool = False,
    acq_label: str = &#34;fdg&#34;,
    suvr_reference_region: str = &#34;pons&#34;,
):
    &#34;&#34;&#34;
    Generates a fully separable dataset.

    Generates a dataset, based on the images of the CAPS directory, where a
    half of the image is processed using a mask to occlude a specific region.
    This procedure creates a dataset fully separable (images with half-right
    processed and image with half-left processed)

    Args:
        caps_directory: path to the CAPS directory.
        output_dir: folder containing the synthetic dataset in CAPS format.
        n_subjects: number of subjects in each class of the synthetic dataset.
        tsv_path: path to tsv file of list of subjects/sessions.
        preprocessing: preprocessing performed. Must be in [&#39;linear&#39;, &#39;extensive&#39;].
        mask_path: path to the extracted masks to generate the two labels.
        atrophy_percent: percentage of atrophy applied.
        multi_cohort: If True caps_directory is the path to a TSV file linking cohort names and paths.
        uncropped_image: If True the uncropped image of `t1-linear` or `pet-linear` will be used.
        acq_label: name of the tracer when using `pet-linear` preprocessing.
        suvr_reference_region: name of the reference region when using `pet-linear` preprocessing.

    Returns:
        Folder structure where images are stored in CAPS format.

    Raises:
        IndexError: if `n_subjects` is higher than the length of the TSV file at `tsv_path`.
    &#34;&#34;&#34;
    from pathlib import Path

    from clinicadl.utils.exceptions import DownloadError

    commandline_to_json(
        {
            &#34;output_dir&#34;: output_dir,
            &#34;caps_dir&#34;: caps_directory,
            &#34;preprocessing&#34;: preprocessing,
            &#34;n_subjects&#34;: n_subjects,
            &#34;atrophy_percent&#34;: atrophy_percent,
        }
    )

    # Transform caps_directory in dict
    caps_dict = CapsDataset.create_caps_dict(caps_directory, multi_cohort=multi_cohort)
    # Read DataFrame
    data_df = load_and_check_tsv(tsv_path, caps_dict, output_dir)
    data_df = extract_baseline(data_df)

    home = str(Path.home())
    cache_clinicadl = join(home, &#34;.cache&#34;, &#34;clinicadl&#34;, &#34;ressources&#34;, &#34;masks&#34;)
    url_aramis = &#34;https://aramislab.paris.inria.fr/files/data/masks/&#34;
    FILE1 = RemoteFileStructure(
        filename=&#34;AAL2.tar.gz&#34;,
        url=url_aramis,
        checksum=&#34;89427970921674792481bffd2de095c8fbf49509d615e7e09e4bc6f0e0564471&#34;,
    )
    makedirs(cache_clinicadl, exist_ok=True)

    if n_subjects &gt; len(data_df):
        raise IndexError(
            f&#34;The number of subjects {n_subjects} cannot be higher &#34;
            f&#34;than the number of subjects in the baseline dataset of size {len(data_df)}&#34;
        )

    if mask_path is None:
        if not exists(join(cache_clinicadl, &#34;AAL2&#34;)):
            print(&#34;Downloading AAL2 masks...&#34;)
            try:
                mask_path_tar = fetch_file(FILE1, cache_clinicadl)
                tar_file = tarfile.open(mask_path_tar)
                print(&#34;File: &#34; + mask_path_tar)
                try:
                    tar_file.extractall(cache_clinicadl)
                    tar_file.close()
                    mask_path = join(cache_clinicadl, &#34;AAL2&#34;)
                except RuntimeError:
                    print(&#34;Unable to extract downloaded files.&#34;)
            except IOError as err:
                print(&#34;Unable to download required templates:&#34;, err)
                raise DownloadError(
                    &#34;&#34;&#34;Unable to download masks, please download them
                    manually at https://aramislab.paris.inria.fr/files/data/masks/
                    and provide a valid path.&#34;&#34;&#34;
                )
        else:
            mask_path = join(cache_clinicadl, &#34;AAL2&#34;)

    # Create subjects dir
    makedirs(join(output_dir, &#34;subjects&#34;), exist_ok=True)

    # Output tsv file
    columns = [&#34;participant_id&#34;, &#34;session_id&#34;, &#34;diagnosis&#34;, &#34;age_bl&#34;, &#34;sex&#34;]
    output_df = pd.DataFrame(columns=columns)
    diagnosis_list = [&#34;AD&#34;, &#34;CN&#34;]

    # Find appropriate preprocessing file type
    file_type = find_file_type(
        preprocessing, uncropped_image, acq_label, suvr_reference_region
    )

    for i in range(2 * n_subjects):
        data_idx = i // 2
        label = i % 2

        participant_id = data_df.loc[data_idx, &#34;participant_id&#34;]
        session_id = data_df.loc[data_idx, &#34;session_id&#34;]
        cohort = data_df.loc[data_idx, &#34;cohort&#34;]
        image_paths = clinica_file_reader(
            [participant_id], [session_id], caps_dict[cohort], file_type
        )[0]
        image_nii = nib.load(image_paths[0])
        image = image_nii.get_data()

        input_filename = basename(image_paths[0])
        filename_pattern = &#34;_&#34;.join(input_filename.split(&#34;_&#34;)[2::])

        trivial_image_nii_dir = join(
            output_dir, &#34;subjects&#34;, f&#34;sub-TRIV{i}&#34;, session_id, preprocessing
        )
        trivial_image_nii_filename = f&#34;sub-TRIV{i}_{session_id}_{filename_pattern}&#34;

        makedirs(trivial_image_nii_dir, exist_ok=True)

        atlas_to_mask = nib.load(join(mask_path, f&#34;mask-{label + 1}.nii&#34;)).get_data()

        # Create atrophied image
        trivial_image = im_loss_roi_gaussian_distribution(
            image, atlas_to_mask, atrophy_percent
        )
        trivial_image_nii = nib.Nifti1Image(trivial_image, affine=image_nii.affine)
        trivial_image_nii.to_filename(
            join(trivial_image_nii_dir, trivial_image_nii_filename)
        )
        print(join(trivial_image_nii_dir, trivial_image_nii_filename))

        # Append row to output tsv
        row = [f&#34;sub-TRIV{i}&#34;, session_id, diagnosis_list[label], 60, &#34;F&#34;]
        row_df = pd.DataFrame([row], columns=columns)
        output_df = output_df.append(row_df)

    output_df.to_csv(join(output_dir, &#34;data.tsv&#34;), sep=&#34;\t&#34;, index=False)

    write_missing_mods(output_dir, output_df)


def generate_shepplogan_dataset(
    output_dir: str,
    img_size: int,
    labels_distribution: Dict[str, Tuple[float, float, float]],
    extract_json: str = None,
    samples: int = 100,
    smoothing: bool = True,
):
    &#34;&#34;&#34;
    Creates a CAPS data set of synthetic data based on Shepp-Logan phantom.
    Source NifTi files are not extracted, but directly the slices as tensors.

    Args:
        output_dir: path to the CAPS created.
        img_size: size of the square image.
        labels_distribution: gives the proportions of the three subtypes (ordered in a tuple) for each label.
        extract_json: name of the JSON file in which generation details are stored.
        samples: number of samples generated per class.
        smoothing: if True, an additional random smoothing is performed on top of all operations on each image.
    &#34;&#34;&#34;

    check_and_clean(join(output_dir, &#34;subjects&#34;))
    commandline_to_json(
        {
            &#34;output_dir&#34;: output_dir,
            &#34;img_size&#34;: img_size,
            &#34;labels_distribution&#34;: labels_distribution,
            &#34;samples&#34;: samples,
            &#34;smoothing&#34;: smoothing,
        }
    )
    columns = [&#34;participant_id&#34;, &#34;session_id&#34;, &#34;diagnosis&#34;, &#34;subtype&#34;]
    data_df = pd.DataFrame(columns=columns)

    for i, label in enumerate(labels_distribution.keys()):
        for j in range(samples):
            participant_id = f&#34;sub-CLNC{i}{j:04d}&#34;
            session_id = &#34;ses-M00&#34;
            subtype = np.random.choice(
                np.arange(len(labels_distribution[label])), p=labels_distribution[label]
            )
            row_df = pd.DataFrame(
                [[participant_id, session_id, label, subtype]], columns=columns
            )
            data_df = data_df.append(row_df)

            # Image generation
            slice_path = join(
                output_dir,
                &#34;subjects&#34;,
                participant_id,
                session_id,
                &#34;deeplearning_prepare_data&#34;,
                &#34;slice_based&#34;,
                &#34;custom&#34;,
                f&#34;{participant_id}_{session_id}_space-SheppLogan_axis-axi_channel-single_slice-0_phantom.pt&#34;,
            )
            slice_dir = dirname(slice_path)
            makedirs(slice_dir, exist_ok=True)

            slice_np = generate_shepplogan_phantom(
                img_size, label=subtype, smoothing=smoothing
            )
            slice_tensor = torch.from_numpy(slice_np).float().unsqueeze(0)
            torch.save(slice_tensor, slice_path)

            image_path = join(
                output_dir,
                &#34;subjects&#34;,
                participant_id,
                session_id,
                &#34;shepplogan&#34;,
                f&#34;{participant_id}_{session_id}_space-SheppLogan_phantom.nii.gz&#34;,
            )
            image_dir = dirname(image_path)
            makedirs(image_dir, exist_ok=True)
            with open(image_path, &#34;w&#34;) as f:
                f.write(&#34;0&#34;)

    # Save data
    data_df.to_csv(join(output_dir, &#34;data.tsv&#34;), sep=&#34;\t&#34;, index=False)

    # Save preprocessing JSON file
    preprocessing_dict = {
        &#34;preprocessing&#34;: &#34;custom&#34;,
        &#34;mode&#34;: &#34;slice&#34;,
        &#34;use_uncropped_image&#34;: False,
        &#34;prepare_dl&#34;: True,
        &#34;extract_json&#34;: compute_extract_json(extract_json),
        &#34;slice_direction&#34;: 2,
        &#34;slice_mode&#34;: &#34;single&#34;,
        &#34;discarded_slices&#34;: 0,
        &#34;num_slices&#34;: 1,
        &#34;file_type&#34;: {
            &#34;pattern&#34;: f&#34;*_space-SheppLogan_phantom.nii.gz&#34;,
            &#34;description&#34;: &#34;Custom suffix&#34;,
            &#34;needed_pipeline&#34;: &#34;shepplogan&#34;,
        },
    }
    write_preprocessing(preprocessing_dict, output_dir)
    write_missing_mods(output_dir, data_df)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="clinicadl.generate.generate.generate_random_dataset"><code class="name flex">
<span>def <span class="ident">generate_random_dataset</span></span>(<span>caps_directory: str, output_dir: str, n_subjects: int, tsv_path: Optional[str] = None, mean: float = 0, sigma: float = 0.5, preprocessing: str = 't1-linear', multi_cohort: bool = False, uncropped_image: bool = False, acq_label: Optional[str] = None, suvr_reference_region: Optional[str] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Generates a random dataset.</p>
<p>Creates a random dataset for intractable classification task from the first
subject of the tsv file (other subjects/sessions different from the first
one are ignored. Degree of noise can be parameterized.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>caps_directory</code></strong></dt>
<dd>Path to the (input) CAPS directory.</dd>
<dt><strong><code>output_dir</code></strong></dt>
<dd>folder containing the synthetic dataset in (output)
CAPS format.</dd>
<dt><strong><code>n_subjects</code></strong></dt>
<dd>number of subjects in each class of the
synthetic dataset</dd>
<dt><strong><code>tsv_path</code></strong></dt>
<dd>path to tsv file of list of subjects/sessions.</dd>
<dt><strong><code>mean</code></strong></dt>
<dd>mean of the gaussian noise</dd>
<dt><strong><code>sigma</code></strong></dt>
<dd>standard deviation of the gaussian noise</dd>
<dt><strong><code>preprocessing</code></strong></dt>
<dd>preprocessing performed. Must be in ['t1-linear', 't1-extensive'].</dd>
<dt><strong><code>multi_cohort</code></strong></dt>
<dd>If True caps_directory is the path to a TSV file linking cohort names and paths.</dd>
<dt><strong><code>uncropped_image</code></strong></dt>
<dd>If True the uncropped image of <code>t1-linear</code> or <code>pet-linear</code> will be used.</dd>
<dt><strong><code>acq_label</code></strong></dt>
<dd>name of the tracer when using <code>pet-linear</code> preprocessing.</dd>
<dt><strong><code>suvr_reference_region</code></strong></dt>
<dd>name of the reference region when using <code>pet-linear</code> preprocessing.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A folder written on the output_dir location (in CAPS format), also a
tsv file describing this output</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_random_dataset(
    caps_directory: str,
    output_dir: str,
    n_subjects: int,
    tsv_path: Optional[str] = None,
    mean: float = 0,
    sigma: float = 0.5,
    preprocessing: str = &#34;t1-linear&#34;,
    multi_cohort: bool = False,
    uncropped_image: bool = False,
    acq_label: Optional[str] = None,
    suvr_reference_region: Optional[str] = None,
):
    &#34;&#34;&#34;
    Generates a random dataset.

    Creates a random dataset for intractable classification task from the first
    subject of the tsv file (other subjects/sessions different from the first
    one are ignored. Degree of noise can be parameterized.

    Args:
        caps_directory: Path to the (input) CAPS directory.
        output_dir: folder containing the synthetic dataset in (output)
            CAPS format.
        n_subjects: number of subjects in each class of the
            synthetic dataset
        tsv_path: path to tsv file of list of subjects/sessions.
        mean: mean of the gaussian noise
        sigma: standard deviation of the gaussian noise
        preprocessing: preprocessing performed. Must be in [&#39;t1-linear&#39;, &#39;t1-extensive&#39;].
        multi_cohort: If True caps_directory is the path to a TSV file linking cohort names and paths.
        uncropped_image: If True the uncropped image of `t1-linear` or `pet-linear` will be used.
        acq_label: name of the tracer when using `pet-linear` preprocessing.
        suvr_reference_region: name of the reference region when using `pet-linear` preprocessing.

    Returns:
        A folder written on the output_dir location (in CAPS format), also a
        tsv file describing this output

    &#34;&#34;&#34;
    commandline_to_json(
        {
            &#34;output_dir&#34;: output_dir,
            &#34;caps_dir&#34;: caps_directory,
            &#34;preprocessing&#34;: preprocessing,
            &#34;n_subjects&#34;: n_subjects,
            &#34;mean&#34;: mean,
            &#34;sigma&#34;: sigma,
        }
    )
    # Transform caps_directory in dict
    caps_dict = CapsDataset.create_caps_dict(caps_directory, multi_cohort=multi_cohort)

    # Read DataFrame
    data_df = load_and_check_tsv(tsv_path, caps_dict, output_dir)

    # Create subjects dir
    makedirs(join(output_dir, &#34;subjects&#34;), exist_ok=True)

    # Retrieve image of first subject
    participant_id = data_df.loc[0, &#34;participant_id&#34;]
    session_id = data_df.loc[0, &#34;session_id&#34;]
    cohort = data_df.loc[0, &#34;cohort&#34;]

    # Find appropriate preprocessing file type
    file_type = find_file_type(
        preprocessing, uncropped_image, acq_label, suvr_reference_region
    )

    image_paths = clinica_file_reader(
        [participant_id], [session_id], caps_dict[cohort], file_type
    )[0]
    image_nii = nib.load(image_paths[0])
    image = image_nii.get_data()

    # Create output tsv file
    participant_id_list = [f&#34;sub-RAND{i}&#34; for i in range(2 * n_subjects)]
    session_id_list = [&#34;ses-M00&#34;] * 2 * n_subjects
    diagnosis_list = [&#34;AD&#34;] * n_subjects + [&#34;CN&#34;] * n_subjects
    data = np.array([participant_id_list, session_id_list, diagnosis_list])
    data = data.T
    output_df = pd.DataFrame(
        data, columns=[&#34;participant_id&#34;, &#34;session_id&#34;, &#34;diagnosis&#34;]
    )
    output_df[&#34;age_bl&#34;] = 60
    output_df[&#34;sex&#34;] = &#34;F&#34;
    output_df.to_csv(join(output_dir, &#34;data.tsv&#34;), sep=&#34;\t&#34;, index=False)

    input_filename = basename(image_paths[0])
    filename_pattern = &#34;_&#34;.join(input_filename.split(&#34;_&#34;)[2::])
    for i in range(2 * n_subjects):
        gauss = np.random.normal(mean, sigma, image.shape)
        participant_id = f&#34;sub-RAND{i}&#34;
        noisy_image = image + gauss
        noisy_image_nii = nib.Nifti1Image(
            noisy_image, header=image_nii.header, affine=image_nii.affine
        )
        noisy_image_nii_path = join(
            output_dir, &#34;subjects&#34;, participant_id, &#34;ses-M00&#34;, &#34;t1_linear&#34;
        )
        noisy_image_nii_filename = f&#34;{participant_id}_ses-M00_{filename_pattern}&#34;
        makedirs(noisy_image_nii_path, exist_ok=True)
        nib.save(noisy_image_nii, join(noisy_image_nii_path, noisy_image_nii_filename))

    write_missing_mods(output_dir, output_df)</code></pre>
</details>
</dd>
<dt id="clinicadl.generate.generate.generate_shepplogan_dataset"><code class="name flex">
<span>def <span class="ident">generate_shepplogan_dataset</span></span>(<span>output_dir: str, img_size: int, labels_distribution: Dict[str, Tuple[float, float, float]], extract_json: str = None, samples: int = 100, smoothing: bool = True)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a CAPS data set of synthetic data based on Shepp-Logan phantom.
Source NifTi files are not extracted, but directly the slices as tensors.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>output_dir</code></strong></dt>
<dd>path to the CAPS created.</dd>
<dt><strong><code>img_size</code></strong></dt>
<dd>size of the square image.</dd>
<dt><strong><code>labels_distribution</code></strong></dt>
<dd>gives the proportions of the three subtypes (ordered in a tuple) for each label.</dd>
<dt><strong><code>extract_json</code></strong></dt>
<dd>name of the JSON file in which generation details are stored.</dd>
<dt><strong><code>samples</code></strong></dt>
<dd>number of samples generated per class.</dd>
<dt><strong><code>smoothing</code></strong></dt>
<dd>if True, an additional random smoothing is performed on top of all operations on each image.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_shepplogan_dataset(
    output_dir: str,
    img_size: int,
    labels_distribution: Dict[str, Tuple[float, float, float]],
    extract_json: str = None,
    samples: int = 100,
    smoothing: bool = True,
):
    &#34;&#34;&#34;
    Creates a CAPS data set of synthetic data based on Shepp-Logan phantom.
    Source NifTi files are not extracted, but directly the slices as tensors.

    Args:
        output_dir: path to the CAPS created.
        img_size: size of the square image.
        labels_distribution: gives the proportions of the three subtypes (ordered in a tuple) for each label.
        extract_json: name of the JSON file in which generation details are stored.
        samples: number of samples generated per class.
        smoothing: if True, an additional random smoothing is performed on top of all operations on each image.
    &#34;&#34;&#34;

    check_and_clean(join(output_dir, &#34;subjects&#34;))
    commandline_to_json(
        {
            &#34;output_dir&#34;: output_dir,
            &#34;img_size&#34;: img_size,
            &#34;labels_distribution&#34;: labels_distribution,
            &#34;samples&#34;: samples,
            &#34;smoothing&#34;: smoothing,
        }
    )
    columns = [&#34;participant_id&#34;, &#34;session_id&#34;, &#34;diagnosis&#34;, &#34;subtype&#34;]
    data_df = pd.DataFrame(columns=columns)

    for i, label in enumerate(labels_distribution.keys()):
        for j in range(samples):
            participant_id = f&#34;sub-CLNC{i}{j:04d}&#34;
            session_id = &#34;ses-M00&#34;
            subtype = np.random.choice(
                np.arange(len(labels_distribution[label])), p=labels_distribution[label]
            )
            row_df = pd.DataFrame(
                [[participant_id, session_id, label, subtype]], columns=columns
            )
            data_df = data_df.append(row_df)

            # Image generation
            slice_path = join(
                output_dir,
                &#34;subjects&#34;,
                participant_id,
                session_id,
                &#34;deeplearning_prepare_data&#34;,
                &#34;slice_based&#34;,
                &#34;custom&#34;,
                f&#34;{participant_id}_{session_id}_space-SheppLogan_axis-axi_channel-single_slice-0_phantom.pt&#34;,
            )
            slice_dir = dirname(slice_path)
            makedirs(slice_dir, exist_ok=True)

            slice_np = generate_shepplogan_phantom(
                img_size, label=subtype, smoothing=smoothing
            )
            slice_tensor = torch.from_numpy(slice_np).float().unsqueeze(0)
            torch.save(slice_tensor, slice_path)

            image_path = join(
                output_dir,
                &#34;subjects&#34;,
                participant_id,
                session_id,
                &#34;shepplogan&#34;,
                f&#34;{participant_id}_{session_id}_space-SheppLogan_phantom.nii.gz&#34;,
            )
            image_dir = dirname(image_path)
            makedirs(image_dir, exist_ok=True)
            with open(image_path, &#34;w&#34;) as f:
                f.write(&#34;0&#34;)

    # Save data
    data_df.to_csv(join(output_dir, &#34;data.tsv&#34;), sep=&#34;\t&#34;, index=False)

    # Save preprocessing JSON file
    preprocessing_dict = {
        &#34;preprocessing&#34;: &#34;custom&#34;,
        &#34;mode&#34;: &#34;slice&#34;,
        &#34;use_uncropped_image&#34;: False,
        &#34;prepare_dl&#34;: True,
        &#34;extract_json&#34;: compute_extract_json(extract_json),
        &#34;slice_direction&#34;: 2,
        &#34;slice_mode&#34;: &#34;single&#34;,
        &#34;discarded_slices&#34;: 0,
        &#34;num_slices&#34;: 1,
        &#34;file_type&#34;: {
            &#34;pattern&#34;: f&#34;*_space-SheppLogan_phantom.nii.gz&#34;,
            &#34;description&#34;: &#34;Custom suffix&#34;,
            &#34;needed_pipeline&#34;: &#34;shepplogan&#34;,
        },
    }
    write_preprocessing(preprocessing_dict, output_dir)
    write_missing_mods(output_dir, data_df)</code></pre>
</details>
</dd>
<dt id="clinicadl.generate.generate.generate_trivial_dataset"><code class="name flex">
<span>def <span class="ident">generate_trivial_dataset</span></span>(<span>caps_directory: str, output_dir: str, n_subjects: int, tsv_path: Optional[str] = None, preprocessing: str = 't1-linear', mask_path: Optional[str] = None, atrophy_percent: float = 60, multi_cohort: bool = False, uncropped_image: bool = False, acq_label: str = 'fdg', suvr_reference_region: str = 'pons')</span>
</code></dt>
<dd>
<div class="desc"><p>Generates a fully separable dataset.</p>
<p>Generates a dataset, based on the images of the CAPS directory, where a
half of the image is processed using a mask to occlude a specific region.
This procedure creates a dataset fully separable (images with half-right
processed and image with half-left processed)</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>caps_directory</code></strong></dt>
<dd>path to the CAPS directory.</dd>
<dt><strong><code>output_dir</code></strong></dt>
<dd>folder containing the synthetic dataset in CAPS format.</dd>
<dt><strong><code>n_subjects</code></strong></dt>
<dd>number of subjects in each class of the synthetic dataset.</dd>
<dt><strong><code>tsv_path</code></strong></dt>
<dd>path to tsv file of list of subjects/sessions.</dd>
<dt><strong><code>preprocessing</code></strong></dt>
<dd>preprocessing performed. Must be in ['linear', 'extensive'].</dd>
<dt><strong><code>mask_path</code></strong></dt>
<dd>path to the extracted masks to generate the two labels.</dd>
<dt><strong><code>atrophy_percent</code></strong></dt>
<dd>percentage of atrophy applied.</dd>
<dt><strong><code>multi_cohort</code></strong></dt>
<dd>If True caps_directory is the path to a TSV file linking cohort names and paths.</dd>
<dt><strong><code>uncropped_image</code></strong></dt>
<dd>If True the uncropped image of <code>t1-linear</code> or <code>pet-linear</code> will be used.</dd>
<dt><strong><code>acq_label</code></strong></dt>
<dd>name of the tracer when using <code>pet-linear</code> preprocessing.</dd>
<dt><strong><code>suvr_reference_region</code></strong></dt>
<dd>name of the reference region when using <code>pet-linear</code> preprocessing.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Folder structure where images are stored in CAPS format.</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>IndexError</code></dt>
<dd>if <code>n_subjects</code> is higher than the length of the TSV file at <code>tsv_path</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_trivial_dataset(
    caps_directory: str,
    output_dir: str,
    n_subjects: int,
    tsv_path: Optional[str] = None,
    preprocessing: str = &#34;t1-linear&#34;,
    mask_path: Optional[str] = None,
    atrophy_percent: float = 60,
    multi_cohort: bool = False,
    uncropped_image: bool = False,
    acq_label: str = &#34;fdg&#34;,
    suvr_reference_region: str = &#34;pons&#34;,
):
    &#34;&#34;&#34;
    Generates a fully separable dataset.

    Generates a dataset, based on the images of the CAPS directory, where a
    half of the image is processed using a mask to occlude a specific region.
    This procedure creates a dataset fully separable (images with half-right
    processed and image with half-left processed)

    Args:
        caps_directory: path to the CAPS directory.
        output_dir: folder containing the synthetic dataset in CAPS format.
        n_subjects: number of subjects in each class of the synthetic dataset.
        tsv_path: path to tsv file of list of subjects/sessions.
        preprocessing: preprocessing performed. Must be in [&#39;linear&#39;, &#39;extensive&#39;].
        mask_path: path to the extracted masks to generate the two labels.
        atrophy_percent: percentage of atrophy applied.
        multi_cohort: If True caps_directory is the path to a TSV file linking cohort names and paths.
        uncropped_image: If True the uncropped image of `t1-linear` or `pet-linear` will be used.
        acq_label: name of the tracer when using `pet-linear` preprocessing.
        suvr_reference_region: name of the reference region when using `pet-linear` preprocessing.

    Returns:
        Folder structure where images are stored in CAPS format.

    Raises:
        IndexError: if `n_subjects` is higher than the length of the TSV file at `tsv_path`.
    &#34;&#34;&#34;
    from pathlib import Path

    from clinicadl.utils.exceptions import DownloadError

    commandline_to_json(
        {
            &#34;output_dir&#34;: output_dir,
            &#34;caps_dir&#34;: caps_directory,
            &#34;preprocessing&#34;: preprocessing,
            &#34;n_subjects&#34;: n_subjects,
            &#34;atrophy_percent&#34;: atrophy_percent,
        }
    )

    # Transform caps_directory in dict
    caps_dict = CapsDataset.create_caps_dict(caps_directory, multi_cohort=multi_cohort)
    # Read DataFrame
    data_df = load_and_check_tsv(tsv_path, caps_dict, output_dir)
    data_df = extract_baseline(data_df)

    home = str(Path.home())
    cache_clinicadl = join(home, &#34;.cache&#34;, &#34;clinicadl&#34;, &#34;ressources&#34;, &#34;masks&#34;)
    url_aramis = &#34;https://aramislab.paris.inria.fr/files/data/masks/&#34;
    FILE1 = RemoteFileStructure(
        filename=&#34;AAL2.tar.gz&#34;,
        url=url_aramis,
        checksum=&#34;89427970921674792481bffd2de095c8fbf49509d615e7e09e4bc6f0e0564471&#34;,
    )
    makedirs(cache_clinicadl, exist_ok=True)

    if n_subjects &gt; len(data_df):
        raise IndexError(
            f&#34;The number of subjects {n_subjects} cannot be higher &#34;
            f&#34;than the number of subjects in the baseline dataset of size {len(data_df)}&#34;
        )

    if mask_path is None:
        if not exists(join(cache_clinicadl, &#34;AAL2&#34;)):
            print(&#34;Downloading AAL2 masks...&#34;)
            try:
                mask_path_tar = fetch_file(FILE1, cache_clinicadl)
                tar_file = tarfile.open(mask_path_tar)
                print(&#34;File: &#34; + mask_path_tar)
                try:
                    tar_file.extractall(cache_clinicadl)
                    tar_file.close()
                    mask_path = join(cache_clinicadl, &#34;AAL2&#34;)
                except RuntimeError:
                    print(&#34;Unable to extract downloaded files.&#34;)
            except IOError as err:
                print(&#34;Unable to download required templates:&#34;, err)
                raise DownloadError(
                    &#34;&#34;&#34;Unable to download masks, please download them
                    manually at https://aramislab.paris.inria.fr/files/data/masks/
                    and provide a valid path.&#34;&#34;&#34;
                )
        else:
            mask_path = join(cache_clinicadl, &#34;AAL2&#34;)

    # Create subjects dir
    makedirs(join(output_dir, &#34;subjects&#34;), exist_ok=True)

    # Output tsv file
    columns = [&#34;participant_id&#34;, &#34;session_id&#34;, &#34;diagnosis&#34;, &#34;age_bl&#34;, &#34;sex&#34;]
    output_df = pd.DataFrame(columns=columns)
    diagnosis_list = [&#34;AD&#34;, &#34;CN&#34;]

    # Find appropriate preprocessing file type
    file_type = find_file_type(
        preprocessing, uncropped_image, acq_label, suvr_reference_region
    )

    for i in range(2 * n_subjects):
        data_idx = i // 2
        label = i % 2

        participant_id = data_df.loc[data_idx, &#34;participant_id&#34;]
        session_id = data_df.loc[data_idx, &#34;session_id&#34;]
        cohort = data_df.loc[data_idx, &#34;cohort&#34;]
        image_paths = clinica_file_reader(
            [participant_id], [session_id], caps_dict[cohort], file_type
        )[0]
        image_nii = nib.load(image_paths[0])
        image = image_nii.get_data()

        input_filename = basename(image_paths[0])
        filename_pattern = &#34;_&#34;.join(input_filename.split(&#34;_&#34;)[2::])

        trivial_image_nii_dir = join(
            output_dir, &#34;subjects&#34;, f&#34;sub-TRIV{i}&#34;, session_id, preprocessing
        )
        trivial_image_nii_filename = f&#34;sub-TRIV{i}_{session_id}_{filename_pattern}&#34;

        makedirs(trivial_image_nii_dir, exist_ok=True)

        atlas_to_mask = nib.load(join(mask_path, f&#34;mask-{label + 1}.nii&#34;)).get_data()

        # Create atrophied image
        trivial_image = im_loss_roi_gaussian_distribution(
            image, atlas_to_mask, atrophy_percent
        )
        trivial_image_nii = nib.Nifti1Image(trivial_image, affine=image_nii.affine)
        trivial_image_nii.to_filename(
            join(trivial_image_nii_dir, trivial_image_nii_filename)
        )
        print(join(trivial_image_nii_dir, trivial_image_nii_filename))

        # Append row to output tsv
        row = [f&#34;sub-TRIV{i}&#34;, session_id, diagnosis_list[label], 60, &#34;F&#34;]
        row_df = pd.DataFrame([row], columns=columns)
        output_df = output_df.append(row_df)

    output_df.to_csv(join(output_dir, &#34;data.tsv&#34;), sep=&#34;\t&#34;, index=False)

    write_missing_mods(output_dir, output_df)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="clinicadl.generate" href="index.html">clinicadl.generate</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="clinicadl.generate.generate.generate_random_dataset" href="#clinicadl.generate.generate.generate_random_dataset">generate_random_dataset</a></code></li>
<li><code><a title="clinicadl.generate.generate.generate_shepplogan_dataset" href="#clinicadl.generate.generate.generate_shepplogan_dataset">generate_shepplogan_dataset</a></code></li>
<li><code><a title="clinicadl.generate.generate.generate_trivial_dataset" href="#clinicadl.generate.generate.generate_trivial_dataset">generate_trivial_dataset</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>