<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>clinicadl.utils.network.autoencoder.cnn_transformer API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>clinicadl.utils.network.autoencoder.cnn_transformer</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from copy import deepcopy

from torch import nn

from clinicadl.utils.network.network_utils import (
    CropMaxUnpool2d,
    CropMaxUnpool3d,
    PadMaxPool2d,
    PadMaxPool3d,
    Reshape,
)


class CNN_Transformer(nn.Module):
    def __init__(self, model=None):
        &#34;&#34;&#34;
        Construct an autoencoder from a given CNN. The encoder part corresponds to the convolutional part of the CNN.

        :param model: (Module) a CNN. The convolutional part must be comprised in a &#39;features&#39; class variable.
        &#34;&#34;&#34;
        from copy import deepcopy

        super(CNN_Transformer, self).__init__()

        self.level = 0

        if model is not None:
            self.encoder = deepcopy(model.convolutions)
            self.decoder = self.construct_inv_layers(model)

            for i, layer in enumerate(self.encoder):
                if isinstance(layer, PadMaxPool3d) or isinstance(layer, PadMaxPool2d):
                    self.encoder[i].set_new_return()
                elif isinstance(layer, nn.MaxPool3d) or isinstance(layer, nn.MaxPool2d):
                    self.encoder[i].return_indices = True
        else:
            self.encoder = nn.Sequential()
            self.decoder = nn.Sequential()

    def __len__(self):
        return len(self.encoder)

    def construct_inv_layers(self, model):
        &#34;&#34;&#34;
        Implements the decoder part from the CNN. The decoder part is the symmetrical list of the encoder
        in which some layers are replaced by their transpose counterpart.
        ConvTranspose and ReLU layers are inverted in the end.

        :param model: (Module) a CNN. The convolutional part must be comprised in a &#39;features&#39; class variable.
        :return: (Module) decoder part of the Autoencoder
        &#34;&#34;&#34;
        inv_layers = []
        for i, layer in enumerate(self.encoder):
            if isinstance(layer, nn.Conv3d):
                inv_layers.append(
                    nn.ConvTranspose3d(
                        layer.out_channels,
                        layer.in_channels,
                        layer.kernel_size,
                        stride=layer.stride,
                        padding=layer.padding,
                    )
                )
                self.level += 1
            elif isinstance(layer, nn.Conv2d):
                inv_layers.append(
                    nn.ConvTranspose2d(
                        layer.out_channels,
                        layer.in_channels,
                        layer.kernel_size,
                        stride=layer.stride,
                        padding=layer.padding,
                    )
                )
                self.level += 1
            elif isinstance(layer, PadMaxPool3d):
                inv_layers.append(
                    CropMaxUnpool3d(layer.kernel_size, stride=layer.stride)
                )
            elif isinstance(layer, PadMaxPool2d):
                inv_layers.append(
                    CropMaxUnpool2d(layer.kernel_size, stride=layer.stride)
                )
            elif isinstance(layer, nn.Linear):
                inv_layers.append(nn.Linear(layer.out_features, layer.in_features))
            elif isinstance(layer, nn.Flatten):
                inv_layers.append(Reshape(model.flattened_shape))
            elif isinstance(layer, nn.LeakyReLU):
                inv_layers.append(nn.LeakyReLU(negative_slope=1 / layer.negative_slope))
            else:
                inv_layers.append(deepcopy(layer))
        inv_layers = self.replace_relu(inv_layers)
        inv_layers.reverse()
        return nn.Sequential(*inv_layers)

    @staticmethod
    def replace_relu(inv_layers):
        &#34;&#34;&#34;
        Invert convolutional and ReLU layers (give empirical better results)

        :param inv_layers: (list) list of the layers of decoder part of the Auto-Encoder
        :return: (list) the layers with the inversion
        &#34;&#34;&#34;
        idx_relu, idx_conv = -1, -1
        for idx, layer in enumerate(inv_layers):
            if isinstance(layer, nn.ConvTranspose3d):
                idx_conv = idx
            elif isinstance(layer, nn.ReLU) or isinstance(layer, nn.LeakyReLU):
                idx_relu = idx

            if idx_conv != -1 and idx_relu != -1:
                inv_layers[idx_relu], inv_layers[idx_conv] = (
                    inv_layers[idx_conv],
                    inv_layers[idx_relu],
                )
                idx_conv, idx_relu = -1, -1

        # Check if number of features of batch normalization layers is still correct
        for idx, layer in enumerate(inv_layers):
            if isinstance(layer, nn.BatchNorm3d):
                conv = inv_layers[idx + 1]
                inv_layers[idx] = nn.BatchNorm3d(conv.out_channels)

        return inv_layers</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="clinicadl.utils.network.autoencoder.cnn_transformer.CNN_Transformer"><code class="flex name class">
<span>class <span class="ident">CNN_Transformer</span></span>
<span>(</span><span>model=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Construct an autoencoder from a given CNN. The encoder part corresponds to the convolutional part of the CNN.</p>
<p>:param model: (Module) a CNN. The convolutional part must be comprised in a 'features' class variable.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CNN_Transformer(nn.Module):
    def __init__(self, model=None):
        &#34;&#34;&#34;
        Construct an autoencoder from a given CNN. The encoder part corresponds to the convolutional part of the CNN.

        :param model: (Module) a CNN. The convolutional part must be comprised in a &#39;features&#39; class variable.
        &#34;&#34;&#34;
        from copy import deepcopy

        super(CNN_Transformer, self).__init__()

        self.level = 0

        if model is not None:
            self.encoder = deepcopy(model.convolutions)
            self.decoder = self.construct_inv_layers(model)

            for i, layer in enumerate(self.encoder):
                if isinstance(layer, PadMaxPool3d) or isinstance(layer, PadMaxPool2d):
                    self.encoder[i].set_new_return()
                elif isinstance(layer, nn.MaxPool3d) or isinstance(layer, nn.MaxPool2d):
                    self.encoder[i].return_indices = True
        else:
            self.encoder = nn.Sequential()
            self.decoder = nn.Sequential()

    def __len__(self):
        return len(self.encoder)

    def construct_inv_layers(self, model):
        &#34;&#34;&#34;
        Implements the decoder part from the CNN. The decoder part is the symmetrical list of the encoder
        in which some layers are replaced by their transpose counterpart.
        ConvTranspose and ReLU layers are inverted in the end.

        :param model: (Module) a CNN. The convolutional part must be comprised in a &#39;features&#39; class variable.
        :return: (Module) decoder part of the Autoencoder
        &#34;&#34;&#34;
        inv_layers = []
        for i, layer in enumerate(self.encoder):
            if isinstance(layer, nn.Conv3d):
                inv_layers.append(
                    nn.ConvTranspose3d(
                        layer.out_channels,
                        layer.in_channels,
                        layer.kernel_size,
                        stride=layer.stride,
                        padding=layer.padding,
                    )
                )
                self.level += 1
            elif isinstance(layer, nn.Conv2d):
                inv_layers.append(
                    nn.ConvTranspose2d(
                        layer.out_channels,
                        layer.in_channels,
                        layer.kernel_size,
                        stride=layer.stride,
                        padding=layer.padding,
                    )
                )
                self.level += 1
            elif isinstance(layer, PadMaxPool3d):
                inv_layers.append(
                    CropMaxUnpool3d(layer.kernel_size, stride=layer.stride)
                )
            elif isinstance(layer, PadMaxPool2d):
                inv_layers.append(
                    CropMaxUnpool2d(layer.kernel_size, stride=layer.stride)
                )
            elif isinstance(layer, nn.Linear):
                inv_layers.append(nn.Linear(layer.out_features, layer.in_features))
            elif isinstance(layer, nn.Flatten):
                inv_layers.append(Reshape(model.flattened_shape))
            elif isinstance(layer, nn.LeakyReLU):
                inv_layers.append(nn.LeakyReLU(negative_slope=1 / layer.negative_slope))
            else:
                inv_layers.append(deepcopy(layer))
        inv_layers = self.replace_relu(inv_layers)
        inv_layers.reverse()
        return nn.Sequential(*inv_layers)

    @staticmethod
    def replace_relu(inv_layers):
        &#34;&#34;&#34;
        Invert convolutional and ReLU layers (give empirical better results)

        :param inv_layers: (list) list of the layers of decoder part of the Auto-Encoder
        :return: (list) the layers with the inversion
        &#34;&#34;&#34;
        idx_relu, idx_conv = -1, -1
        for idx, layer in enumerate(inv_layers):
            if isinstance(layer, nn.ConvTranspose3d):
                idx_conv = idx
            elif isinstance(layer, nn.ReLU) or isinstance(layer, nn.LeakyReLU):
                idx_relu = idx

            if idx_conv != -1 and idx_relu != -1:
                inv_layers[idx_relu], inv_layers[idx_conv] = (
                    inv_layers[idx_conv],
                    inv_layers[idx_relu],
                )
                idx_conv, idx_relu = -1, -1

        # Check if number of features of batch normalization layers is still correct
        for idx, layer in enumerate(inv_layers):
            if isinstance(layer, nn.BatchNorm3d):
                conv = inv_layers[idx + 1]
                inv_layers[idx] = nn.BatchNorm3d(conv.out_channels)

        return inv_layers</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="clinicadl.utils.network.autoencoder.cnn_transformer.CNN_Transformer.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="clinicadl.utils.network.autoencoder.cnn_transformer.CNN_Transformer.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="clinicadl.utils.network.autoencoder.cnn_transformer.CNN_Transformer.replace_relu"><code class="name flex">
<span>def <span class="ident">replace_relu</span></span>(<span>inv_layers)</span>
</code></dt>
<dd>
<div class="desc"><p>Invert convolutional and ReLU layers (give empirical better results)</p>
<p>:param inv_layers: (list) list of the layers of decoder part of the Auto-Encoder
:return: (list) the layers with the inversion</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def replace_relu(inv_layers):
    &#34;&#34;&#34;
    Invert convolutional and ReLU layers (give empirical better results)

    :param inv_layers: (list) list of the layers of decoder part of the Auto-Encoder
    :return: (list) the layers with the inversion
    &#34;&#34;&#34;
    idx_relu, idx_conv = -1, -1
    for idx, layer in enumerate(inv_layers):
        if isinstance(layer, nn.ConvTranspose3d):
            idx_conv = idx
        elif isinstance(layer, nn.ReLU) or isinstance(layer, nn.LeakyReLU):
            idx_relu = idx

        if idx_conv != -1 and idx_relu != -1:
            inv_layers[idx_relu], inv_layers[idx_conv] = (
                inv_layers[idx_conv],
                inv_layers[idx_relu],
            )
            idx_conv, idx_relu = -1, -1

    # Check if number of features of batch normalization layers is still correct
    for idx, layer in enumerate(inv_layers):
        if isinstance(layer, nn.BatchNorm3d):
            conv = inv_layers[idx + 1]
            inv_layers[idx] = nn.BatchNorm3d(conv.out_channels)

    return inv_layers</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="clinicadl.utils.network.autoencoder.cnn_transformer.CNN_Transformer.construct_inv_layers"><code class="name flex">
<span>def <span class="ident">construct_inv_layers</span></span>(<span>self, model)</span>
</code></dt>
<dd>
<div class="desc"><p>Implements the decoder part from the CNN. The decoder part is the symmetrical list of the encoder
in which some layers are replaced by their transpose counterpart.
ConvTranspose and ReLU layers are inverted in the end.</p>
<p>:param model: (Module) a CNN. The convolutional part must be comprised in a 'features' class variable.
:return: (Module) decoder part of the Autoencoder</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def construct_inv_layers(self, model):
    &#34;&#34;&#34;
    Implements the decoder part from the CNN. The decoder part is the symmetrical list of the encoder
    in which some layers are replaced by their transpose counterpart.
    ConvTranspose and ReLU layers are inverted in the end.

    :param model: (Module) a CNN. The convolutional part must be comprised in a &#39;features&#39; class variable.
    :return: (Module) decoder part of the Autoencoder
    &#34;&#34;&#34;
    inv_layers = []
    for i, layer in enumerate(self.encoder):
        if isinstance(layer, nn.Conv3d):
            inv_layers.append(
                nn.ConvTranspose3d(
                    layer.out_channels,
                    layer.in_channels,
                    layer.kernel_size,
                    stride=layer.stride,
                    padding=layer.padding,
                )
            )
            self.level += 1
        elif isinstance(layer, nn.Conv2d):
            inv_layers.append(
                nn.ConvTranspose2d(
                    layer.out_channels,
                    layer.in_channels,
                    layer.kernel_size,
                    stride=layer.stride,
                    padding=layer.padding,
                )
            )
            self.level += 1
        elif isinstance(layer, PadMaxPool3d):
            inv_layers.append(
                CropMaxUnpool3d(layer.kernel_size, stride=layer.stride)
            )
        elif isinstance(layer, PadMaxPool2d):
            inv_layers.append(
                CropMaxUnpool2d(layer.kernel_size, stride=layer.stride)
            )
        elif isinstance(layer, nn.Linear):
            inv_layers.append(nn.Linear(layer.out_features, layer.in_features))
        elif isinstance(layer, nn.Flatten):
            inv_layers.append(Reshape(model.flattened_shape))
        elif isinstance(layer, nn.LeakyReLU):
            inv_layers.append(nn.LeakyReLU(negative_slope=1 / layer.negative_slope))
        else:
            inv_layers.append(deepcopy(layer))
    inv_layers = self.replace_relu(inv_layers)
    inv_layers.reverse()
    return nn.Sequential(*inv_layers)</code></pre>
</details>
</dd>
<dt id="clinicadl.utils.network.autoencoder.cnn_transformer.CNN_Transformer.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, *input: Any) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def _forward_unimplemented(self, *input: Any) -&gt; None:
    r&#34;&#34;&#34;Defines the computation performed at every call.

    Should be overridden by all subclasses.

    .. note::
        Although the recipe for forward pass needs to be defined within
        this function, one should call the :class:`Module` instance afterwards
        instead of this since the former takes care of running the
        registered hooks while the latter silently ignores them.
    &#34;&#34;&#34;
    raise NotImplementedError(f&#34;Module [{type(self).__name__}] is missing the required \&#34;forward\&#34; function&#34;)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="clinicadl.utils.network.autoencoder" href="index.html">clinicadl.utils.network.autoencoder</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="clinicadl.utils.network.autoencoder.cnn_transformer.CNN_Transformer" href="#clinicadl.utils.network.autoencoder.cnn_transformer.CNN_Transformer">CNN_Transformer</a></code></h4>
<ul class="">
<li><code><a title="clinicadl.utils.network.autoencoder.cnn_transformer.CNN_Transformer.construct_inv_layers" href="#clinicadl.utils.network.autoencoder.cnn_transformer.CNN_Transformer.construct_inv_layers">construct_inv_layers</a></code></li>
<li><code><a title="clinicadl.utils.network.autoencoder.cnn_transformer.CNN_Transformer.dump_patches" href="#clinicadl.utils.network.autoencoder.cnn_transformer.CNN_Transformer.dump_patches">dump_patches</a></code></li>
<li><code><a title="clinicadl.utils.network.autoencoder.cnn_transformer.CNN_Transformer.forward" href="#clinicadl.utils.network.autoencoder.cnn_transformer.CNN_Transformer.forward">forward</a></code></li>
<li><code><a title="clinicadl.utils.network.autoencoder.cnn_transformer.CNN_Transformer.replace_relu" href="#clinicadl.utils.network.autoencoder.cnn_transformer.CNN_Transformer.replace_relu">replace_relu</a></code></li>
<li><code><a title="clinicadl.utils.network.autoencoder.cnn_transformer.CNN_Transformer.training" href="#clinicadl.utils.network.autoencoder.cnn_transformer.CNN_Transformer.training">training</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>