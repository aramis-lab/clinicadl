# CONFIG FILE FOR TRAIN PIPELINE WITH DEFAULT ARGUMENTS


[Model]
architecture = "default" # ex : Conv5_FC3
multi_network = false

[Architecture]
# CNN
dropout = 0.0 # between 0 and 1
# architecture.n_layers = 4
# VAE
latent_space_size = 128
feature_size = 1024
n_conv = 4
io_layer_channels = 8
recons_weight = 1
kl_weight = 1
normalization = "batch"

[Classification]
selection_metrics = ["loss"]
label = "diagnosis"
label_code = {}
selection_threshold = 0.0 # Will only be used if num_networks != 1
loss = "CrossEntropyLoss"

[Regression]
selection_metrics = ["loss"]
label = "age"
loss = "MSELoss"

[Reconstruction]
selection_metrics = ["loss"]
loss = "MSELoss"

[Pythae]
# Beta VAE
beta=5
# Linear Normalizing Flow VAE
flows=['Planar', 'Radial', 'Planar']
# Inverse Autoregressive Flows
n_made_blocks=2
n_hidden_in_made=3
hidden_size=128
# Disentangled Beta VAE
#beta=5
C=30.0
warmup_epoch=25
# Factor VAE
gamma=10
# Beta TC VAE
#beta=2.
alpha=1
#gamma=1
# MS SSIM VAE
#beta=1e-2
window_size=3
# Info VAE
kernel_choice='imq'
#alpha=-2
lbd=10
kernel_bandwidth=1
# Wasserstein Autoencoder
#kernel_choice='imq'
reg_weight=100
#kernel_bandwidth=2
# Hyperspherical VAE
# Poincare VAE
reconstruction_loss="bce"
prior_distribution="riemannian_normal"
posterior_distribution="wrapped_normal"
curvature=0.7
# Adversarial AE
adversarial_loss_scale=0.9
# VAE GAN
#adversarial_loss_scale=0.8
reconstruction_layer=3
margin=0.4
equilibrium= 0.68
# VQ VAE
commitment_loss_factor=0.25
quantization_loss_factor=1.0
num_embeddings=128
use_ema=true
decay=0.99
# Hamiltonian VAE
n_lf=1
eps_lf=0.001
beta_zero=0.3
# Riemannian Hamiltonian VAE
#n_lf=1
#eps_lf=0.001
#beta_zero=0.3
temperature=1.5
regularization=0.001
# Importance Weighted Autoencoder
number_samples=3
# Multiply Importance Weighted Autoencoder
number_gradient_estimates=4
#number_samples=4
# Partially Importance Weighted Autoencoder
#number_gradient_estimates=4
#number_samples=4
# Combination Importance Weighted Autoencoder
#beta=0.05
#number_samples=4
# VAMP Autoencoder
number_components=50
# Regularized AE with L2 decoder param
embedding_weight=1e-2
#reg_weight=1e-4
# Regularized AE with gradient penalty
#embedding_weight=1e-2
#reg_weight=1e-4

[Computational]
gpu = true
n_proc = 2
batch_size = 8
evaluation_steps = 0

[Reproducibility]
seed = 0
deterministic = false
compensation = "memory" # Only used if deterministic = true

[Transfer_learning]
transfer_path = ""
transfer_selection_metric = "loss"

[Mode]
# require to manually generate preprocessing json
use_extracted_features = false

[Data]
multi_cohort = false
diagnoses = ["AD", "CN"]
baseline = false
normalize = true
data_augmentation = false
sampler = "random"
size_reduction=false
size_reduction_factor=2

[Cross_validation]
n_splits = 0
split = []

[Optimization]
optimizer = "Adam"
epochs = 20
learning_rate = 1e-4
weight_decay = 1e-4
patience = 0
tolerance = 0.0
accumulation_steps = 1
